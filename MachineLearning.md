# Machine Learning 吴恩达  
## 一 监督学习和无监督学习  
### 1-1 监督学习  
监督学习*Supervied Learning*：有正确的答案  
分类问题*Regression Problem*：预测持续的数据输出  
回归问题：预测离散值输出  
### 1-2 无监督学习  
无监督学习：给大量数据，找出数据的类型结构  
聚类算法：  
应用：新闻，组织大型计算机集群，社交网络的分析，市场细分的应用，天文数据分析  
应用：鸡尾酒排队问题  
## 二 函数和梯度下降  
### 2-1 模型描述    
#### 一些变量的定义  
m：训练样本的数量  
x：输入特征  
y：输出变量  
(x，y)：表示一个训练样本  
(x^(i), y^(i))：特定训练样本, i只是索引  
h假设函数*hypothesis*  
线性回归*Linear regression*  
### 2-2~4 代价函数  
#### 代价函数是什么  
代价函数*Cost Function*：平方误差函数，目的是要让代价函数最小，就是方差最小，则假设函数h更拟合原数据  
![](2-2.2.jpg)  
#### 代价函数的解释（1）
![](2-3.1.jpg)  
这里先把假设函数h简化了，只讨论西塔1而不讨论西塔0  

![](2-3.3.jpg)  
左图是代价函数的几何解释：对任意西塔，假设函数与原函数的差值的平方即为代价函数在该西塔处的值，其图像为右图。  
当西塔取1时，假设函数与原函数能拟合。  
#### 代价函数的解释（2）  
当同时讨论西塔1和西塔0时，普通的二维图像难以表达它的几何意义  
![](2-4.1.jpg)  
如上图，这就可以用一个三维图像来表达，以西塔0和西塔1这两个自变量的图像  
![](2-4.2.jpg)  
![](2-4.3.jpg)  
上面两图是用等高图绘制的图像，同一椭圆线上的点即表示他们的J相同  
由上面两图就可看出，图二的函数可以较好拟合  
### 2-5 梯度下降  
#### 梯度下降是什么  
![](2-5.1.jpg)  
梯度下降：是一种用来寻找最小值的算法  
梯度下降的思想：随机取一点，然后在它周围找一点比它小的点，再在周围找一个更小的点……直到找到局部最小值的点  
#### 梯度下降的解释（1）  
![](2-5.2.jpg)  
![](2-5.3.jpg)  
用三维图像来解释，即随机寻找一点，接着找比它小的点，接着再找更小的点……直到找到最小的点。如图，运用梯度下降算法找到了J的两个最小值的点，说明这种算法存在缺陷。  
#### 梯度下降的解释（2）  
![](2-5.4.jpg)
*“:=”：用来表示赋值；*  
*阿尔法：学习率，用来控制梯度下降时的变化率；*  
用数学公式来解释，先用这个公式求出temp0和temp1并且分别赋值给西塔0和西塔1，然后一直更新西塔0和西塔1……  
错误的方法：求出temp0赋值给西塔0，然后求出temp1赋值给西塔1……  

